{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\pablo\\anaconda3\\lib\\site-packages (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\pablo\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 212\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# Selecione e execute uma ação\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     action \u001b[38;5;241m=\u001b[39m select_action(state, epsilon)\n\u001b[1;32m--> 212\u001b[0m     next_state, reward, done, score \u001b[38;5;241m=\u001b[39m take_action(snake, direction, action, score, food)\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# Observe o novo estado\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     replay_buffer\u001b[38;5;241m.\u001b[39mpush(state, action, reward, next_state, done)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Configurações do jogo\n",
    "LARGURA, ALTURA = 640, 480\n",
    "TAMANHO_DA_SERPENTE = 20\n",
    "VELOCIDADE = 5\n",
    "\n",
    "# Configurações de aprendizado por reforço\n",
    "GAMMA = 0.99\n",
    "EPSILON_INICIAL = 0.9\n",
    "EPSILON_FINAL = 0.05\n",
    "EPSILON_DECAIMENTO = 200\n",
    "REPLAY_BUFFER_CAPACIDADE = 10000\n",
    "BATCH_SIZE = 32\n",
    "num_episodes = 1000\n",
    "\n",
    "# Função para verificar colisões\n",
    "def is_collision(point1, point2, tamanho=TAMANHO_DA_SERPENTE):\n",
    "    # Verifica se as coordenadas x e y de dois pontos estão dentro de 'tamanho' uma da outra\n",
    "    return abs(point1[0] - point2[0]) < tamanho and abs(point1[1] - point2[1]) < tamanho\n",
    "\n",
    "# Rede Neural\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Buffer de Replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Função de seleção de ação\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor([state], dtype=torch.float32)\n",
    "            q_values = q_network(state)\n",
    "            action = q_values.max(1)[1].item()\n",
    "    else:\n",
    "        action = random.randrange(4)\n",
    "    return action\n",
    "\n",
    "def get_state(snake, direction, food):\n",
    "    head = snake[0]\n",
    "    point_l = (head[0] - TAMANHO_DA_SERPENTE, head[1])\n",
    "    point_r = (head[0] + TAMANHO_DA_SERPENTE, head[1])\n",
    "    point_u = (head[0], head[1] - TAMANHO_DA_SERPENTE)\n",
    "    point_d = (head[0], head[1] + TAMANHO_DA_SERPENTE)\n",
    "\n",
    "    dir_l = direction == 'ESQUERDA'\n",
    "    dir_r = direction == 'DIREITA'\n",
    "    dir_u = direction == 'CIMA'\n",
    "    dir_d = direction == 'BAIXO'\n",
    "\n",
    "    state = [\n",
    "        # Perigo direto à frente\n",
    "        (dir_r and any(is_collision(point_r, part) for part in snake[1:])) or \n",
    "        (dir_l and any(is_collision(point_l, part) for part in snake[1:])) or \n",
    "        (dir_u and any(is_collision(point_u, part) for part in snake[1:])) or \n",
    "        (dir_d and any(is_collision(point_d, part) for part in snake[1:])),\n",
    "\n",
    "        # Perigo direto à direita\n",
    "        (dir_u and any(is_collision(point_r, part) for part in snake[1:])) or \n",
    "        (dir_d and any(is_collision(point_l, part) for part in snake[1:])) or \n",
    "        (dir_l and any(is_collision(point_u, part) for part in snake[1:])) or \n",
    "        (dir_r and any(is_collision(point_d, part) for part in snake[1:])),\n",
    "\n",
    "        # Perigo direto à esquerda\n",
    "        (dir_d and any(is_collision(point_r, part) for part in snake[1:])) or \n",
    "        (dir_u and any(is_collision(point_l, part) for part in snake[1:])) or \n",
    "        (dir_r and any(is_collision(point_u, part) for part in snake[1:])) or \n",
    "        (dir_l and any(is_collision(point_d, part) for part in snake[1:])),\n",
    "\n",
    "        # Movimento direcional\n",
    "        dir_l,\n",
    "        dir_r,\n",
    "        dir_u,\n",
    "        dir_d,\n",
    "\n",
    "        # Posição da comida\n",
    "        food[0] < head[0],  # Comida à esquerda\n",
    "        food[0] > head[0],  # Comida à direita\n",
    "        food[1] < head[1],  # Comida acima\n",
    "        food[1] > head[1]   # Comida abaixo\n",
    "    ]\n",
    "\n",
    "    return np.array(state, dtype=int)\n",
    "\n",
    "def take_action(snake, direction, action, score, food):\n",
    "    # Atualiza a direção com base na ação\n",
    "    new_direction = direction\n",
    "    if action == 0:  # CIMA\n",
    "        if direction != 'BAIXO':\n",
    "            new_direction = 'CIMA'\n",
    "    elif action == 1:  # BAIXO\n",
    "        if direction != 'CIMA':\n",
    "            new_direction = 'BAIXO'\n",
    "    elif action == 2:  # ESQUERDA\n",
    "        if direction != 'DIREITA':\n",
    "            new_direction = 'ESQUERDA'\n",
    "    elif action == 3:  # DIREITA\n",
    "        if direction != 'ESQUERDA':\n",
    "            new_direction = 'DIREITA'\n",
    "\n",
    "    # Atualiza a posição da serpente se a direção nova for diferente da atual\n",
    "    if new_direction != direction:\n",
    "        direction = new_direction\n",
    "        x, y = snake[0]\n",
    "        if direction == 'CIMA':\n",
    "            y -= TAMANHO_DA_SERPENTE\n",
    "        elif direction == 'BAIXO':\n",
    "            y += TAMANHO_DA_SERPENTE\n",
    "        elif direction == 'ESQUERDA':\n",
    "            x -= TAMANHO_DA_SERPENTE\n",
    "        elif direction == 'DIREITA':\n",
    "            x += TAMANHO_DA_SERPENTE\n",
    "        new_head = (x, y)\n",
    "    else:\n",
    "        # Se a direção não mudar, continua na mesma direção\n",
    "        new_head = snake[0]\n",
    "\n",
    "    # Verifica colisão com as bordas\n",
    "    done = x < 0 or x >= LARGURA or y < 0 or y >= ALTURA\n",
    "\n",
    "    # Verifica colisão consigo mesma\n",
    "    if new_head in snake[1:]:  # A cabeça não pode colidir com o primeiro segmento (ela mesma)\n",
    "        done = True\n",
    "\n",
    "    # Verifica se a comida foi consumida\n",
    "    eat = new_head == food\n",
    "    if eat:\n",
    "        score += 1\n",
    "        food = (random.randint(0, (LARGURA - TAMANHO_DA_SERPENTE) // TAMANHO_DA_SERPENTE) * TAMANHO_DA_SERPENTE,\n",
    "                random.randint(0, (ALTURA - TAMANHO_DA_SERPENTE) // TAMANHO_DA_SERPENTE) * TAMANHO_DA_SERPENTE)\n",
    "    else:\n",
    "        # Remove o último segmento do corpo se a comida não foi consumida\n",
    "        snake.pop()\n",
    "\n",
    "    # Adiciona a nova cabeça à serpente\n",
    "    snake.insert(0, new_head)\n",
    "\n",
    "    # Define a recompensa\n",
    "    reward = 0\n",
    "    if eat:\n",
    "        reward = 10\n",
    "    elif done:\n",
    "        reward = -10\n",
    "\n",
    "    # Obtém o próximo estado\n",
    "    next_state = get_state(snake, direction, food)\n",
    "\n",
    "    return next_state, reward, done, score, food\n",
    "\n",
    "# Função de perda TD\n",
    "def compute_td_loss(batch_size):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.uint8)\n",
    "\n",
    "    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_values = q_network(next_states).max(1)[0]\n",
    "    expected_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(q_values, expected_q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Inicialização do Pygame e preparação do ambiente\n",
    "pygame.init()\n",
    "tela = pygame.display.set_mode((LARGURA, ALTURA))\n",
    "pygame.display.set_caption('Snake Game')\n",
    "clock = pygame.time.Clock()\n",
    "fonte = pygame.font.SysFont('arial', 25)\n",
    "\n",
    "# Inicialização do agente e do buffer de replay\n",
    "q_network = QNetwork(11, 256, 4)\n",
    "optimizer = optim.Adam(q_network.parameters())\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACIDADE)\n",
    "epsilon = EPSILON_INICIAL\n",
    "\n",
    "# Nome do arquivo onde o estado do modelo será salvo\n",
    "MODEL_FILENAME = 'q_network.pth'\n",
    "# Verifica se um modelo treinado já existe e carrega-o\n",
    "if os.path.isfile(MODEL_FILENAME):\n",
    "    q_network.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    q_network.eval()  # coloca a rede em modo de avaliação\n",
    "    # Se necessário, também carregue o valor de epsilon aqui\n",
    "    # epsilon = ...\n",
    "\n",
    "# Loop principal do jogo e treinamento\n",
    "for episode in range(num_episodes):\n",
    "    snake = [(LARGURA//2, ALTURA//2)]\n",
    "    direction = 'DIREITA'\n",
    "    food = (random.randint(0, (LARGURA-TAMANHO_DA_SERPENTE)//TAMANHO_DA_SERPENTE) * TAMANHO_DA_SERPENTE,\n",
    "            random.randint(0, (ALTURA-TAMANHO_DA_SERPENTE)//TAMANHO_DA_SERPENTE) * TAMANHO_DA_SERPENTE)\n",
    "    score = 0\n",
    "    state = get_state(snake, direction, food)\n",
    "    for t in count():\n",
    "        # Selecione e execute uma ação\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, score, food = take_action(snake, direction, action, score, food)\n",
    "\n",
    "        # Observe o novo estado\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        # Renderização e atualização da tela\n",
    "        tela.fill((0, 0, 0))\n",
    "        for part in snake:\n",
    "            pygame.draw.rect(tela, pygame.Color('green'), pygame.Rect(part[0], part[1], TAMANHO_DA_SERPENTE, TAMANHO_DA_SERPENTE))\n",
    "        pygame.draw.rect(tela, pygame.Color('red'), pygame.Rect(food[0], food[1], TAMANHO_DA_SERPENTE, TAMANHO_DA_SERPENTE))\n",
    "        text = fonte.render(\"Score: {}\".format(score), True, pygame.Color('white'))\n",
    "        tela.blit(text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "        clock.tick(VELOCIDADE)\n",
    "        \n",
    "        # Aprenda com a experiência (Replay Buffer)\n",
    "        loss = compute_td_loss(BATCH_SIZE)\n",
    "\n",
    "        # Salva o modelo após cada episódio ou após um número definido de episódios\n",
    "        if episode % 100 == 0:  # Aqui estamos salvando a cada 100 episódios\n",
    "            torch.save(q_network.state_dict(), MODEL_FILENAME)\n",
    "            # Se necessário, também salve o valor de epsilon aqui\n",
    "            # ...\n",
    "\n",
    "        if done:\n",
    "            break        \n",
    "\n",
    "    # Atualize a política de exploração (epsilon)\n",
    "    if epsilon > EPSILON_FINAL:\n",
    "        epsilon -= (EPSILON_INICIAL - EPSILON_FINAL) / EPSILON_DECAIMENTO\n",
    "\n",
    "    print(f\"Episódio: {episode}, Score: {score}, Perda: {loss}\")\n",
    "\n",
    "    # salvar o modelo uma última vez após o treinamento ser concluído\n",
    "    torch.save(q_network.state_dict(), MODEL_FILENAME)\n",
    "\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
